{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8662e7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>candidate</th>\n",
       "      <th>candidate_confidence</th>\n",
       "      <th>relevant_yn</th>\n",
       "      <th>relevant_yn_confidence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_confidence</th>\n",
       "      <th>subject_matter</th>\n",
       "      <th>subject_matter_confidence</th>\n",
       "      <th>candidate_gold</th>\n",
       "      <th>...</th>\n",
       "      <th>relevant_yn_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>sentiment_gold</th>\n",
       "      <th>subject_matter_gold</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.6578</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @NancyLeeGrahn: How did everyone feel about...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:54:46 -0700</td>\n",
       "      <td>629697200650592256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Quito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Scott Walker</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @ScottWalker: Didn't catch the full #GOPdeb...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:54:46 -0700</td>\n",
       "      <td>629697199560069120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.6629</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>0.6629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @TJMShow: No mention of Tamir Rice and the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:54:46 -0700</td>\n",
       "      <td>629697199312482304</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>0.7039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @RobGeorge: That Carly Fiorina is trending ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:54:45 -0700</td>\n",
       "      <td>629697197118861312</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.7045</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @DanScavino: #GOPDebate w/ @realDonaldTrump...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:54:45 -0700</td>\n",
       "      <td>629697196967903232</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13866</th>\n",
       "      <td>13867</td>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.7991</td>\n",
       "      <td>Abortion</td>\n",
       "      <td>0.6014</td>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>7</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Abortion\\nWomen's Issues (not abortion though)</td>\n",
       "      <td>RT @cappy_yarbrough: Love to see men who will ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:29:43 -0700</td>\n",
       "      <td>629690895479250944</td>\n",
       "      <td>Como</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13867</th>\n",
       "      <td>13868</td>\n",
       "      <td>Mike Huckabee</td>\n",
       "      <td>0.9611</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.7302</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>0.9229</td>\n",
       "      <td>Mike Huckabee</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @georgehenryw: Who thought Huckabee exceede...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:25:02 -0700</td>\n",
       "      <td>629689719056568320</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13868</th>\n",
       "      <td>13869</td>\n",
       "      <td>Ted Cruz</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.8051</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>0.9647</td>\n",
       "      <td>Ted Cruz</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>67</td>\n",
       "      <td>Positive\\nNeutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @Lrihendry: #TedCruz As President, I will a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 07:19:18 -0700</td>\n",
       "      <td>629658075784282112</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13869</th>\n",
       "      <td>13870</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Women's Issues (not abortion though)</td>\n",
       "      <td>0.9202</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Women's Issues (not abortion though)</td>\n",
       "      <td>RT @JRehling: #GOPDebate Donald Trump says tha...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:54:04 -0700</td>\n",
       "      <td>629697023663546368</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13870</th>\n",
       "      <td>13871</td>\n",
       "      <td>Ted Cruz</td>\n",
       "      <td>0.9242</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.9614</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.9614</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>0.9242</td>\n",
       "      <td>Ted Cruz</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>65</td>\n",
       "      <td>Positive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @Lrihendry: #TedCruz headed into the Presid...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-06 18:22:27 -0700</td>\n",
       "      <td>629462573641920512</td>\n",
       "      <td>San Antonio, TX</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13871 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id               candidate  candidate_confidence relevant_yn  \\\n",
       "0          1  No candidate mentioned                1.0000         yes   \n",
       "1          2            Scott Walker                1.0000         yes   \n",
       "2          3  No candidate mentioned                1.0000         yes   \n",
       "3          4  No candidate mentioned                1.0000         yes   \n",
       "4          5            Donald Trump                1.0000         yes   \n",
       "...      ...                     ...                   ...         ...   \n",
       "13866  13867  No candidate mentioned                1.0000         yes   \n",
       "13867  13868           Mike Huckabee                0.9611         yes   \n",
       "13868  13869                Ted Cruz                1.0000         yes   \n",
       "13869  13870            Donald Trump                1.0000         yes   \n",
       "13870  13871                Ted Cruz                0.9242         yes   \n",
       "\n",
       "       relevant_yn_confidence sentiment  sentiment_confidence  \\\n",
       "0                      1.0000   Neutral                0.6578   \n",
       "1                      1.0000  Positive                0.6333   \n",
       "2                      1.0000   Neutral                0.6629   \n",
       "3                      1.0000  Positive                1.0000   \n",
       "4                      1.0000  Positive                0.7045   \n",
       "...                       ...       ...                   ...   \n",
       "13866                  1.0000  Negative                0.7991   \n",
       "13867                  1.0000  Positive                0.7302   \n",
       "13868                  1.0000  Positive                0.8051   \n",
       "13869                  1.0000  Negative                1.0000   \n",
       "13870                  0.9614  Positive                0.9614   \n",
       "\n",
       "                             subject_matter  subject_matter_confidence  \\\n",
       "0                         None of the above                     1.0000   \n",
       "1                         None of the above                     1.0000   \n",
       "2                         None of the above                     0.6629   \n",
       "3                         None of the above                     0.7039   \n",
       "4                         None of the above                     1.0000   \n",
       "...                                     ...                        ...   \n",
       "13866                              Abortion                     0.6014   \n",
       "13867                     None of the above                     0.9229   \n",
       "13868                     None of the above                     0.9647   \n",
       "13869  Women's Issues (not abortion though)                     0.9202   \n",
       "13870                     None of the above                     0.9242   \n",
       "\n",
       "               candidate_gold  ... relevant_yn_gold retweet_count  \\\n",
       "0                         NaN  ...              NaN             5   \n",
       "1                         NaN  ...              NaN            26   \n",
       "2                         NaN  ...              NaN            27   \n",
       "3                         NaN  ...              NaN           138   \n",
       "4                         NaN  ...              NaN           156   \n",
       "...                       ...  ...              ...           ...   \n",
       "13866  No candidate mentioned  ...              yes             7   \n",
       "13867           Mike Huckabee  ...              yes             1   \n",
       "13868                Ted Cruz  ...              yes            67   \n",
       "13869            Donald Trump  ...              yes           149   \n",
       "13870                Ted Cruz  ...              yes            65   \n",
       "\n",
       "          sentiment_gold                             subject_matter_gold  \\\n",
       "0                    NaN                                             NaN   \n",
       "1                    NaN                                             NaN   \n",
       "2                    NaN                                             NaN   \n",
       "3                    NaN                                             NaN   \n",
       "4                    NaN                                             NaN   \n",
       "...                  ...                                             ...   \n",
       "13866           Negative  Abortion\\nWomen's Issues (not abortion though)   \n",
       "13867                NaN                                             NaN   \n",
       "13868  Positive\\nNeutral                                             NaN   \n",
       "13869                NaN            Women's Issues (not abortion though)   \n",
       "13870           Positive                                             NaN   \n",
       "\n",
       "                                                    text tweet_coord  \\\n",
       "0      RT @NancyLeeGrahn: How did everyone feel about...         NaN   \n",
       "1      RT @ScottWalker: Didn't catch the full #GOPdeb...         NaN   \n",
       "2      RT @TJMShow: No mention of Tamir Rice and the ...         NaN   \n",
       "3      RT @RobGeorge: That Carly Fiorina is trending ...         NaN   \n",
       "4      RT @DanScavino: #GOPDebate w/ @realDonaldTrump...         NaN   \n",
       "...                                                  ...         ...   \n",
       "13866  RT @cappy_yarbrough: Love to see men who will ...         NaN   \n",
       "13867  RT @georgehenryw: Who thought Huckabee exceede...         NaN   \n",
       "13868  RT @Lrihendry: #TedCruz As President, I will a...         NaN   \n",
       "13869  RT @JRehling: #GOPDebate Donald Trump says tha...         NaN   \n",
       "13870  RT @Lrihendry: #TedCruz headed into the Presid...         NaN   \n",
       "\n",
       "                   tweet_created            tweet_id   tweet_location  \\\n",
       "0      2015-08-07 09:54:46 -0700  629697200650592256              NaN   \n",
       "1      2015-08-07 09:54:46 -0700  629697199560069120              NaN   \n",
       "2      2015-08-07 09:54:46 -0700  629697199312482304              NaN   \n",
       "3      2015-08-07 09:54:45 -0700  629697197118861312            Texas   \n",
       "4      2015-08-07 09:54:45 -0700  629697196967903232              NaN   \n",
       "...                          ...                 ...              ...   \n",
       "13866  2015-08-07 09:29:43 -0700  629690895479250944             Como   \n",
       "13867  2015-08-07 09:25:02 -0700  629689719056568320              USA   \n",
       "13868  2015-08-07 07:19:18 -0700  629658075784282112              NaN   \n",
       "13869  2015-08-07 09:54:04 -0700  629697023663546368              NaN   \n",
       "13870  2015-08-06 18:22:27 -0700  629462573641920512  San Antonio, TX   \n",
       "\n",
       "                    user_timezone  \n",
       "0                           Quito  \n",
       "1                             NaN  \n",
       "2                             NaN  \n",
       "3      Central Time (US & Canada)  \n",
       "4                         Arizona  \n",
       "...                           ...  \n",
       "13866                         NaN  \n",
       "13867                         NaN  \n",
       "13868                         NaN  \n",
       "13869                         NaN  \n",
       "13870  Central Time (US & Canada)  \n",
       "\n",
       "[13871 rows x 21 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "x=pd.read_csv(\"C:/Users/hemap/Downloads/Paper2/Sentiment.csv\")\n",
    "df=pd.DataFrame(x)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ace4f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13871, 21)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc28bfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[['text','sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f7d4f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e11c3399",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316d5697",
   "metadata": {},
   "source": [
    "## Q1. Print the total number of positive and negative sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe95a4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['sentiment']=='Neutral'].index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f02a229",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9dd5073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Negative    6129\n",
       "Positive    1709\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13acfc6a",
   "metadata": {},
   "source": [
    "## Q2. Build a sequential LSTM model to predict positive and negative sentiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "89901710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hemap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3edfe902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7838\n"
     ]
    }
   ],
   "source": [
    "word_corpus = []\n",
    "for text in df['text']:\n",
    "    words = [word.lower() for word in word_tokenize(text)] \n",
    "    word_corpus.append(words)\n",
    "numberof_words = len(word_corpus)\n",
    "print(numberof_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4740541d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@', 'georgehenryw', ':', 'Who', 'thought', 'Huckabee', 'exceeded', 'their', 'expectations', '#', 'gopdebate', '#', 'imwithhuck', '#', 'gop', '#', 'ccot', '#', 'teaparty', '#', 'tcot', '@', 'laura4fairtax', 'http…']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "token=word_tokenize(text)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "369a3260",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist=FreqDist(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "26b8836a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['RT', 'georgehenryw']),\n",
       " WordList(['georgehenryw', 'Who']),\n",
       " WordList(['Who', 'thought']),\n",
       " WordList(['thought', 'Huckabee']),\n",
       " WordList(['Huckabee', 'exceeded']),\n",
       " WordList(['exceeded', 'their']),\n",
       " WordList(['their', 'expectations']),\n",
       " WordList(['expectations', 'gopdebate']),\n",
       " WordList(['gopdebate', 'imwithhuck']),\n",
       " WordList(['imwithhuck', 'gop']),\n",
       " WordList(['gop', 'ccot']),\n",
       " WordList(['ccot', 'teaparty']),\n",
       " WordList(['teaparty', 'tcot']),\n",
       " WordList(['tcot', 'laura4fairtax']),\n",
       " WordList(['laura4fairtax', 'http…'])]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import textblob\n",
    "from textblob import TextBlob\n",
    "TextBlob(text).ngrams(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67240279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst=PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "lst=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cd52b783",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hemap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "05bb88d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rt', '@', 'georgehenryw', ':', 'who', 'thought', 'huckabee', 'exceeded', 'their', 'expectations', '#', 'gopdebate', '#', 'imwithhuck', '#', 'gop', '#', 'ccot', '#', 'teaparty', '#', 'tcot', '@', 'laura4fairtax', 'http…']\n"
     ]
    }
   ],
   "source": [
    "a=set(stopwords.words('english'))\n",
    "t=word_tokenize(text.lower())\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "14544b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['R', 'T', ' ', '@', 'g', 'e', 'r', 'g', 'e', 'h', 'e', 'n', 'r', 'w', ':', ' ', 'W', 'h', ' ', 'h', 'u', 'g', 'h', ' ', 'H', 'u', 'c', 'k', 'b', 'e', 'e', ' ', 'e', 'x', 'c', 'e', 'e', 'e', ' ', 'h', 'e', 'r', ' ', 'e', 'x', 'p', 'e', 'c', 'n', ' ', ' ', '\\n', '\\n', '#', 'g', 'p', 'e', 'b', 'e', ' ', '#', 'w', 'h', 'h', 'u', 'c', 'k', ' ', '#', 'g', 'p', ' ', '#', 'c', 'c', ' ', '#', 'e', 'p', 'r', ' ', '#', 'c', '\\n', '@', 'l', 'u', 'r', '4', 'f', 'r', 'x', ' ', 'h', 'p', '…']\n"
     ]
    }
   ],
   "source": [
    "stopwords=[x for x in text if x not in a]\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8f389848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "all_words= ' '.join([t for t in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aee7efd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c4dae688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.9.1-cp39-cp39-win_amd64.whl (444.0 MB)\n",
      "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "Collecting tensorboard<2.10,>=2.9\n",
      "  Downloading tensorboard-2.9.0-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hemap\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\hemap\\anaconda3\\lib\\site-packages (from tensorflow) (3.2.1)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.46.3-cp39-cp39-win_amd64.whl (3.5 MB)\n",
      "Collecting flatbuffers<2,>=1.12\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\hemap\\anaconda3\\lib\\site-packages (from tensorflow) (21.0)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.4-cp39-cp39-win_amd64.whl (895 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\hemap\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in c:\\users\\hemap\\anaconda3\\lib\\site-packages (from tensorflow) (2.9.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.1-py2.py3-none-win_amd64.whl (14.2 MB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\hemap\\anaconda3\\lib\\site-packages (from tensorflow) (1.20.3)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hemap\\anaconda3\\lib\\site-packages (from tensorflow) (58.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\hemap\\anaconda3\\lib\\site-packages (from tensorflow) (3.10.0.2)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hemap\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hemap\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.0.2)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.6.6-py2.py3-none-any.whl (156 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hemap\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.26.0)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.1.0-py3-none-any.whl (9.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\hemap\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hemap\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.6.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hemap\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hemap\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hemap\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hemap\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.2)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\hemap\\anaconda3\\lib\\site-packages (from packaging->tensorflow) (3.0.4)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=847983a6cba8b07e6909a631f2f41563c26696f4ad4881f624778d2e1e9b8b1e\n",
      "  Stored in directory: c:\\users\\hemap\\appdata\\local\\pip\\cache\\wheels\\b6\\0d\\90\\0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\n",
      "Successfully built termcolor\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.1.0 flatbuffers-1.12 gast-0.4.0 google-auth-2.6.6 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.46.3 keras-preprocessing-1.1.2 libclang-14.0.1 markdown-3.3.7 oauthlib-3.2.0 opt-einsum-3.3.0 protobuf-3.19.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.9.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.26.0 termcolor-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "127a6254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import keras\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0394cd13",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    st = \"\"\n",
    "    for w in w_tokenizer.tokenize(text):\n",
    "        st = st + lemmatizer.lemmatize(w) + \" \"\n",
    "    return st\n",
    "df['text'] = df.text.apply(lemmatize_text)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173931a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0.0\n",
    "for i in df['text']:\n",
    "    word_list = i.split()\n",
    "    s = s + len(word_list)\n",
    "print(\"Average length of each text : \",s/df.shape[0])\n",
    "pos = 0\n",
    "for i in range(df.shape[0]):\n",
    "    if df.iloc[i]['sentiment'] == 'positive':\n",
    "        pos = pos + 1\n",
    "neg = df.shape[0]-pos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2144a243",
   "metadata": {},
   "source": [
    "## Q3 Checkthe Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afab3d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer=Tokenizer(num_words=5000)\n",
    "\n",
    "sentence = ['He is a great leader','He is a terrible leader.' ]\n",
    "# convert to a sequence\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "# pad the sequence\n",
    "padded = pad_sequences(sequences, padding='post', maxlen=max_length)\n",
    "# Get labels based on probability 1 if p>= 0.5 else 0\n",
    "prediction = model.predict(padded)\n",
    "pred_labels = []\n",
    "for i in prediction:\n",
    "    if i >= 0.5:\n",
    "        pred_labels.append(1)\n",
    "    else:\n",
    "        pred_labels.append(0)\n",
    "for i in range(len(sentence)):\n",
    "    print(sentence[i])\n",
    "    if pred_labels[i] == 1:\n",
    "        s = 'Positive'\n",
    "    else:\n",
    "        s = 'Negative'\n",
    "    print(\"Predicted sentiment : \",s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
